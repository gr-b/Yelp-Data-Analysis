{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 2 : Data Science in Yelp Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required Readings:** \n",
    "* [Yelp Dataset Challenge](https://www.yelp.com/dataset_challenge) \n",
    "* Please download the Yelp dataset from the above webpage.\n",
    "* [TED Talks](https://www.ted.com/talks) for examples of 10 minutes talks.\n",
    "\n",
    "\n",
    "** NOTE **\n",
    "* Please don't forget to save the notebook frequently when working in Jupyter Notebook, otherwise the changes you made can be lost.\n",
    "\n",
    "*----------------------\n",
    "\n",
    "Here is an example of the data format. More details are included [here](https://www.yelp.com/dataset_challenge)\n",
    "\n",
    "## Business Objects\n",
    "\n",
    "Business objects contain basic information about local businesses. The fields are as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "  'type': 'business',\n",
    "  'business_id': (a unique identifier for this business),\n",
    "  'name': (the full business name),\n",
    "  'neighborhoods': (a list of neighborhood names, might be empty),\n",
    "  'full_address': (localized address),\n",
    "  'city': (city),\n",
    "  'state': (state),\n",
    "  'latitude': (latitude),\n",
    "  'longitude': (longitude),\n",
    "  'stars': (star rating, rounded to half-stars),\n",
    "  'review_count': (review count),\n",
    "  'photo_url': (photo url),\n",
    "  'categories': [(localized category names)]\n",
    "  'open': (is the business still open for business?),\n",
    "  'schools': (nearby universities),\n",
    "  'url': (yelp url)\n",
    "}\n",
    "```\n",
    "## Checkin Objects\n",
    "```json\n",
    "{\n",
    "    'type': 'checkin',\n",
    "    'business_id': (encrypted business id),\n",
    "    'checkin_info': {\n",
    "        '0-0': (number of checkins from 00:00 to 01:00 on all Sundays),\n",
    "        '1-0': (number of checkins from 01:00 to 02:00 on all Sundays),\n",
    "        ...\n",
    "        '14-4': (number of checkins from 14:00 to 15:00 on all Thursdays),\n",
    "        ...\n",
    "        '23-6': (number of checkins from 23:00 to 00:00 on all Saturdays)\n",
    "    }, # if there was no checkin for a hour-day block it will not be in the dict\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: pick a data science problem that you plan to solve using Yelp Data\n",
    "* The problem should be important and interesting, which has a potential impact in some area.\n",
    "* The problem should be solvable using yelp data and data science solutions.\n",
    "\n",
    "Please briefly describe in the following cell: what problem are you trying to solve? why this problem is important and interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "In the previous case study, several groups performed sentiment analysis on tweets using dictionaries of words given\n",
    "weights towards a positive or negative sentiment. To create such a dictionary, one must usually use advanced machine learning \n",
    "techniques. Yelp provides a unique solution to this problem; if one assumes that reviews with higher ratings will have a larger \n",
    "concentration of words with a positive sentiment, and that reviews with lower ratings will have a larger concentration of words \n",
    "with a negative sentiment, then it is possible to use this data to discover which words/phrases have a positive/negative\n",
    "sentiment. \n",
    "\n",
    "This data would be also usefull for any business which is heavily reviewed on Yelp; it would provide a scalable way to know which\n",
    "things they could improve on--which things have been mentioned in a negative sentiment, and which good things they are known for.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection/Processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The data was downloaded from the Yelp Dataset challenge website. We used MRJob to process a portion of this data.\n",
    "# We just downloaded the data so there was no collection code involved\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration: Exploring the Yelp Dataset\n",
    "\n",
    "**(1) Finding the most popular business categories:** \n",
    "* print the top 10 most popular business categories in the dataset and their counts (i.e., how many business objects in each category). Here we say a category is \"popular\" if there are many business objects in this category (such as 'restaurants')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json, prettytable\n",
    "\n",
    "data = open('yelp_academic_dataset_business.json','r')\n",
    "\n",
    "categ = {}\n",
    "\n",
    "#i = 0\n",
    "for line in data:\n",
    "    business = json.loads(line.strip())\n",
    "    categories = business['categories']\n",
    "    if categories is None:\n",
    "        continue\n",
    "    for category in categories:\n",
    "        if category not in categ.keys():\n",
    "            categ[category] = 1\n",
    "        else:\n",
    "            categ[category] += 1\n",
    "    #print json.dumps(business, indent=2)\n",
    "    #i +=1\n",
    "    #if i >100:\n",
    "    #    break\n",
    "\n",
    "#Get the top ten most frequent categories\n",
    "categories = categ.items()\n",
    "sorted_categories = sorted(categories, key=lambda x: x[1], reverse=True)\n",
    "#print sorted_categories[0:10]\n",
    "table = prettytable.PrettyTable(['Category','Count of businesses in category'])\n",
    "for row in sorted_categories[0:10]:\n",
    "    table.add_row(row)\n",
    "print table\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "+------------------+---------------------------------+\n",
    "|     Category     | Count of businesses in category |\n",
    "+------------------+---------------------------------+\n",
    "|   Restaurants    |              48485              |\n",
    "|     Shopping     |              22466              |\n",
    "|       Food       |              21189              |\n",
    "|  Beauty & Spas   |              13711              |\n",
    "|  Home Services   |              11241              |\n",
    "|    Nightlife     |              10524              |\n",
    "| Health & Medical |              10476              |\n",
    "|       Bars       |               9087              |\n",
    "|    Automotive    |               8554              |\n",
    "|  Local Services  |               8133              |\n",
    "+------------------+---------------------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (2) Find the most popular business objects** \n",
    "* print the top 10 most popular business objects in the dataset and their counts (i.e., how many checkins in total for each business object).  Here we say a business object is \"popular\" if the business object attracts a large number of checkins from the users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Go through the ...checkin.json file, counting the number of checkins for each business id.\n",
    "2) Go through the ...business.json file, creating a lookup table so that we can convert each business id to a business name\n",
    "3) Use the lookup table to convert business ids to names and finally,\n",
    "4) Display the table with counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# WARNING: This takes a very long time. Results have been \n",
    "# recorded below.\n",
    "import json, prettytable\n",
    "\n",
    "data = open('yelp_academic_dataset_checkin.json','r')\n",
    "\n",
    "business_id_counts = {}\n",
    "\n",
    "i = 0\n",
    "for line in data:\n",
    "    checkin = json.loads(line.strip())\n",
    "    business_id = checkin['business_id']\n",
    "    if business_id is not None:\n",
    "        if business_id not in business_id_counts.keys():\n",
    "            business_id_counts[business_id] = len(checkin['time'])\n",
    "        else:\n",
    "            business_id_counts[business_id] += len(checkin['time'])\n",
    "    #print json.dumps(business, indent=2)\n",
    "    i +=1\n",
    "    #if i >10000:\n",
    "    #    break\n",
    "    print str(i), 'counted.\\r',\n",
    "\n",
    "#Get the top ten most frequent categories\n",
    "business_counts = business_id_counts.items()\n",
    "sorted_counts = sorted(business_counts, key=lambda x: x[1], reverse=True)\n",
    "table = prettytable.PrettyTable(['Business ID','Count'])\n",
    "out_counts = {}\n",
    "for row in sorted_counts[0:10]:\n",
    "    table.add_row(row)\n",
    "    out_counts[row[0]] = row[1]\n",
    "\n",
    "with open('checkin_table.txt', 'w') as outfile:\n",
    "    outfile.write(table)\n",
    "\n",
    "with open('checkin_table.json', 'w') as outfile:\n",
    "    outfile.write(json.dumps(out_counts, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generated table: \n",
    "    +------------------------+-------+ | Business ID | Count | +------------------------+-------+ \n",
    "    | FaHADZARwnY4yvlvpnsfGA | 168 | \n",
    "    | t-o_Sraneime4DDhWrQRBA | 168 | \n",
    "    | VxCnyVYn-FFgv6F1EqbdKA | 168 | \n",
    "    | -kG0N8sBhBotMbu0KVSPaw | 168 | \n",
    "    | na4Th5DrNauOv-c43QQFvA | 168 | \n",
    "    | gRCEObNuHtI61xR32ytqNQ | 168 | \n",
    "    | O3lQvyOADBs7f2W8A5D0Yg | 168 | \n",
    "    | jKmAswXvFVRHN4VP-88zOA | 168 | \n",
    "    | u_vPjx925UPEG9DFOAAvFQ | 168 | \n",
    "    | XXW_OFaYQkkGOGniujZFHg | 168 | \n",
    "    +------------------------+-------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated table:\n",
    "+------------------------+-------+\n",
    "|      Business ID       | Count |\n",
    "+------------------------+-------+\n",
    "| FaHADZARwnY4yvlvpnsfGA |  168  |\n",
    "| t-o_Sraneime4DDhWrQRBA |  168  |\n",
    "| VxCnyVYn-FFgv6F1EqbdKA |  168  |\n",
    "| -kG0N8sBhBotMbu0KVSPaw |  168  |\n",
    "| na4Th5DrNauOv-c43QQFvA |  168  |\n",
    "| gRCEObNuHtI61xR32ytqNQ |  168  |\n",
    "| O3lQvyOADBs7f2W8A5D0Yg |  168  |\n",
    "| jKmAswXvFVRHN4VP-88zOA |  168  |\n",
    "| u_vPjx925UPEG9DFOAAvFQ |  168  |\n",
    "| XXW_OFaYQkkGOGniujZFHg |  168  |\n",
    "+------------------------+-------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a lookup table for business ids\n",
    "import json, prettytable\n",
    "\n",
    "business_lookup = {}\n",
    "\n",
    "data = open('yelp_academic_dataset_business.json','r')\n",
    "for line in data:\n",
    "    business = json.loads(line.strip())\n",
    "    business_id_str = business['business_id']\n",
    "    business_name_str = business['name']\n",
    "    \n",
    "    business_lookup[business_id_str] = business_name_str\n",
    "    \n",
    "with open('business_lookup.json','w') as lookup_file:\n",
    "    lookup_file.write(json.dumps(business_lookup, indent=2))\n",
    "print 'Writing Complete.'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have created our lookup table and stored it in business_lookup.json\n",
    "We have also created a table of the most popular businesses(in terms of ids)\n",
    "All we need to do is load both tables in, and lookup each id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, prettytable\n",
    "\n",
    "# The file to decode each business id\n",
    "with open('business_lookup.json','r') as lookup_file:\n",
    "    read = lookup_file.read().strip()\n",
    "    lookup_table = json.loads(read)\n",
    "\n",
    "# The file with the top ten business ids and counts\n",
    "with open('checkin_table.json','r') as checkin_file:\n",
    "    read = checkin_file.read().strip()\n",
    "    checkin_table = json.loads(read)\n",
    "\n",
    "final_table = {}\n",
    "\n",
    "for business_id in checkin_table.keys():\n",
    "    business_name = lookup_table[business_id]\n",
    "    business_count = checkin_table[business_id]\n",
    "\n",
    "    final_table[business_name] = business_count\n",
    "\n",
    "table = prettytable.PrettyTable(['Business Name','Checkin Count'])\n",
    "sorted_items = sorted(final_table.items(), key=lambda x: x[1], reverse=True)\n",
    "for row in sorted_items:\n",
    "    table.add_row(row)\n",
    "\n",
    "with open('business_count_table.txt', 'w') as outfile:\n",
    "    outfile.write(str(table))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "+---------------------------------------------+---------------+ | Business Name | Checkin Count | +---------------------------------------------+---------------+\n",
    "| Bellagio Hotel | 168 | \n",
    "| Toronto Pearson International Airport | 168 | \n",
    "| 24 Hour Fitness Sport | 168 | \n",
    "| Gold Coast Hotel & Casino | 168 | \n",
    "| Mandarin Oriental, Las Vegas | 168 | \n",
    "| Palace Station Hotel & Casino | 168 | \n",
    "| Wynn Las Vegas | 168 | \n",
    "| McCarran International Airport | 168 | \n",
    "| The Peppermill Restaurant & Fireside Lounge | 168 | \n",
    "| Flamingo Las Vegas Hotel & Casino | 168 |\n",
    "+---------------------------------------------+---------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+---------------------------------------------+---------------+\n",
    "|                Business Name                | Checkin Count |\n",
    "+---------------------------------------------+---------------+\n",
    "|                Bellagio Hotel               |      168      |\n",
    "|    Toronto Pearson International Airport    |      168      |\n",
    "|            24 Hour Fitness Sport            |      168      |\n",
    "|          Gold Coast Hotel & Casino          |      168      |\n",
    "|         Mandarin Oriental, Las Vegas        |      168      |\n",
    "|        Palace Station Hotel & Casino        |      168      |\n",
    "|                Wynn Las Vegas               |      168      |\n",
    "|        McCarran International Airport       |      168      |\n",
    "| The Peppermill Restaurant & Fireside Lounge |      168      |\n",
    "|      Flamingo Las Vegas Hotel & Casino      |      168      |\n",
    "+---------------------------------------------+---------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Solution: implement a data science solution to the problem you are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly describe the idea of your solution to the problem in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Reverse Sentiment Analysis\n",
    "In order to figure out the sentiment (positive, neutral, negative) of any given word, we used the following process:\n",
    "    1. Go through each review and remove all punctuation and grammar (anything but alphabetical characters). Convert to lowercase.\n",
    "    2. Remove all irrelevant parts of speech (coordinating conjuctives, prepositions, etc)\n",
    "    3. For each word in the review left, record the score of the review for that word\n",
    "    4. When all reviews have been processed, take the average of the scores for each word. Record this as the words final score.\n",
    "    \n",
    "    Using this simple preliminary analysis, words with a higher score will be used in more positive reviews, and words with a lower \n",
    "    score will have been used in more negative reviews.\n",
    "    \n",
    "We can then use this data to predict the score for any given review. By taking the average score for each word in a review, we can\n",
    "get a somewhat accurate prediction. The graph in the figures section shows how effective this method is.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write codes to implement the solution in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This code is run on the command line with the input being piped in from the review file.\n",
    "from mrjob.job import MRJob\n",
    "import json, nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "undesirables = ['CC','IN','TO','DT']\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "class MRWordScore(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        review = json.loads(line.strip())\n",
    "        cleaned_text = clean_text(review['text'])\n",
    "        clean_words = cleaned_text.split()\n",
    "        cleaner_text = remove_unecessary_pos(cleaned_text).split()\n",
    "        #stemmed_words = [stemmer.stem(word) for word in cleaner_text.split()]\n",
    "        score = float(review['stars'])\n",
    "        for word in cleaner_text:\n",
    "            yield (word, score)\n",
    "\n",
    "    '''def combiner(self, word, counts):\n",
    "        length = len(list(counts))\n",
    "        if length == 0:\n",
    "            yield (word, 0)\n",
    "        yield (word, int(sum(counts)/length)) # Average score'''\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        total = 0\n",
    "        count = 0\n",
    "        for num in counts:\n",
    "            total += num\n",
    "            count += 1\n",
    "        #if count == 0:\n",
    "        #    yield (word, (0, count))\n",
    "        if count < 10:\n",
    "            return\n",
    "        yield (word, {'score':total/count, 'count':count}) # Average score\n",
    "\n",
    "def clean_text(text):\n",
    "    # Get rid of all bad characters, and uppercase.\n",
    "    new_text = ''\n",
    "    for c in text:\n",
    "        if c in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':\n",
    "            new_text += c.lower()\n",
    "        elif c in 'abcdefghijklmnopqrstuvwxyz ':\n",
    "            new_text += c\n",
    "    return new_text\n",
    "\n",
    "def remove_unecessary_pos(text):\n",
    "    pos_list = nltk.pos_tag(text.split())\n",
    "    #print pos_list\n",
    "    new_text = ''\n",
    "    for pos in pos_list:\n",
    "        if pos[1] in undesirables:\n",
    "            pass\n",
    "        else:\n",
    "            new_text += pos[0] + ' '\n",
    "    return new_text\n",
    "\n",
    "\n",
    "#dataset = open('yelp_academic_dataset_review.json','r')\n",
    "\n",
    "#clean_reviews = []\n",
    "\n",
    "def write_data(clean_reviews):\n",
    "    with open('clean_reviews.json','a') as outfile:\n",
    "        print 'Writing {0} reviews to file.'.format(len(clean_reviews))\n",
    "        [outfile.write(json.dumps(review) + '\\n') for review in clean_reviews]\n",
    "        #outfile.write(json.dumps(clean_reviews))\n",
    "        outfile.close()\n",
    "\n",
    "MRWordScore.run()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used to predict review scores\n",
    "import sys, json, math, nltk, random\n",
    "undesirables = ['CC','IN','TO','DT']\n",
    "\n",
    "def log(n):\n",
    "    return math.log(n) - 1.3\n",
    "\n",
    "def calcscore(score):\n",
    "    if score < 24.17:\n",
    "        return 1\n",
    "    elif score < 24.75:\n",
    "        return 2\n",
    "    elif score < 25.2:\n",
    "        return 3\n",
    "    elif score < 25.45:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "words = {}\n",
    "wordlist = open(sys.argv[1],'r')\n",
    "for line in wordlist:\n",
    "    firstspace = line.strip().index('{')\n",
    "    word = line[0:firstspace][1:-2]\n",
    "    info = line[firstspace-1:]\n",
    "    infoDict = json.loads(info.strip())\n",
    "    score = infoDict['score']\n",
    "    count = infoDict['count']\n",
    "    infoDict['score'] = score*log(count)\n",
    "    words[word] = infoDict\n",
    "    #print word, '{0:.2f}'.format(infoDict['score'])\n",
    "\n",
    "'''sorted_words = sorted(words, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "for wordDict in sorted_words:\n",
    "    score = wordDict['score']\n",
    "    count = wordDict['count']\n",
    "    print wordDict['word'], '{0:.2f}'.format(wordDict['score'])'''\n",
    "\n",
    "def clean_text(text):\n",
    "    # Get rid of all bad characters, and uppercase.\n",
    "    new_text = ''\n",
    "    for c in text:\n",
    "        if c in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':\n",
    "            new_text += c.lower()\n",
    "        elif c in 'abcdefghijklmnopqrstuvwxyz ':\n",
    "            new_text += c\n",
    "    return new_text\n",
    "\n",
    "def remove_unecessary_pos(text):\n",
    "    pos_list = nltk.pos_tag(text.split())\n",
    "    #print pos_list\n",
    "    new_text = ''\n",
    "    for pos in pos_list:\n",
    "        if pos[1] in undesirables:\n",
    "            pass\n",
    "        else:\n",
    "            new_text += pos[0] + ' '\n",
    "    return new_text\n",
    "\n",
    "\n",
    "scores = {}\n",
    "\n",
    "i = 0\n",
    "for line in open(sys.argv[2], 'r'):\n",
    "    review = json.loads(line.strip())\n",
    "    cleaned_text = clean_text(review['text'])\n",
    "    cleaner_words = remove_unecessary_pos(cleaned_text).split()\n",
    "    total = 0\n",
    "    count = 0\n",
    "    for word in cleaner_words:\n",
    "        if word in words.keys():\n",
    "            total += words[word]['score']\n",
    "            count += 1\n",
    "        if count == 0:\n",
    "            print 'Continuing ' + word\n",
    "    if count == 0:\n",
    "        continue\n",
    "    score = total/count\n",
    "    print 'Our score: {0:.2f} | Their score: {1}'.format(int(calcscore(score)), review['stars'])\n",
    "    #scores += (score, int(review['stars']))\n",
    "    if review['stars'] not in scores.keys():\n",
    "        scores[review['stars']] = []\n",
    "    scores[review['stars']] += [score]\n",
    "    i += 1\n",
    "    if i > 50:\n",
    "        break\n",
    "\n",
    "for key, scorelist in scores.items():\n",
    "    total = 0\n",
    "    for score in scorelist:\n",
    "        total += score\n",
    "    print 'Average for {} : {}'.format(key, total/len(scorelist))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results: summarize and visualize the results discovered from the analysis\n",
    "\n",
    "Please use figures, tables, or videos to communicate the results with the audience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Most positive words (with > 100 uses)\n",
    "+-------------+-----------------------+\n",
    "|     Word    | Score (Average stars) |\n",
    "+-------------+-----------------------+\n",
    "|   crossfit  |          4.81         |\n",
    "|   listens   |          4.81         |\n",
    "|    debbie   |          4.75         |\n",
    "|  meticulous |          4.73         |\n",
    "|  supportive |          4.70         |\n",
    "|   talented  |          4.66         |\n",
    "|   painless  |          4.66         |\n",
    "| trustworthy |          4.66         |\n",
    "|   workouts  |          4.63         |\n",
    "|    kelly    |          4.63         |\n",
    "+-------------+-----------------------+\n",
    "Most negative words (with > 100 uses)\n",
    "+----------------+-----------------------+\n",
    "|      Word      | Score (Average stars) |\n",
    "+----------------+-----------------------+\n",
    "| unprofessional |          1.23         |\n",
    "|     rudely     |          1.32         |\n",
    "|  incompetent   |          1.32         |\n",
    "|      scam      |          1.33         |\n",
    "|     refund     |          1.37         |\n",
    "|   dishonest    |          1.41         |\n",
    "|   disgusted    |          1.44         |\n",
    "|   unhelpful    |          1.44         |\n",
    "|      bbb       |          1.44         |\n",
    "|    apology     |          1.44         |\n",
    "+----------------+-----------------------+\n",
    "Most neutral words (with > 100 uses)\n",
    "+-----------+-----------------------+\n",
    "|    Word   | Score (Average stars) |\n",
    "+-----------+-----------------------+\n",
    "|   boxes   |          3.48         |\n",
    "|    here   |          3.48         |\n",
    "|    size   |          3.48         |\n",
    "|   forget  |          3.48         |\n",
    "|  toppings |          3.48         |\n",
    "|    does   |          3.48         |\n",
    "|  squeeze  |          3.49         |\n",
    "| croissant |          3.49         |\n",
    "|    gast   |          3.49         |\n",
    "|   bench   |          3.49         |\n",
    "+-----------+-----------------------+\n",
    "\n",
    "Please also see the graph in this same folder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this Jupyter notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"jupyter notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . Each team present their case studies in class for 10 minutes.\n",
    "\n",
    "Please compress all the files in a zipped file.\n",
    "\n",
    "\n",
    "** How to submit: **\n",
    "\n",
    "        Please submit through Canvas, in the Assignment \"Case Study 2\".\n",
    "        \n",
    "** Note: Each team only needs to submit one submission in Canvas **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Peer-Review Grading Template:\n",
    "\n",
    "** Total Points: (100 points) ** Please don't worry about the absolute scores, we will rescale the final grading according to the performance of all teams in the class.\n",
    "\n",
    "Please add an \"**X**\" mark in front of your rating: \n",
    "\n",
    "For example:\n",
    "\n",
    "*2: bad*\n",
    "          \n",
    "**X** *3: good*\n",
    "    \n",
    "*4: perfect*\n",
    "\n",
    "\n",
    "    ---------------------------------\n",
    "    The Problem: \n",
    "    ---------------------------------\n",
    "    \n",
    "    1. (10 points) how well did the team describe the problem they are trying to solve using the data? \n",
    "       0: not clear\n",
    "       2: I can barely understand the problem\n",
    "       4: okay, can be improved\n",
    "       6: good, but can be improved\n",
    "       8: very good\n",
    "       10: crystal clear\n",
    "    \n",
    "    2. (10 points) do you think the problem is important or has a potential impact?\n",
    "        0: not important at all\n",
    "        2: not sure if it is important\n",
    "        4: seems important, but not clear\n",
    "        6: interesting problem\n",
    "        8: an important problem, which I want to know the answer myself\n",
    "       10: very important, I would be happy invest money on a project like this.\n",
    "    \n",
    "    ----------------------------------\n",
    "    Data Collection and Processing:\n",
    "    ----------------------------------\n",
    "    \n",
    "    3. (10 points) Do you think the data collected/processed are relevant and sufficient for solving the above problem? \n",
    "       0: not clear\n",
    "       2: I can barely understand what data they are trying to collect/process\n",
    "       4: I can barely understand why the data is relevant to the problem\n",
    "       6: the data are relevant to the problem, but better data can be collected\n",
    "       8: the data collected are relevant and at a proper scale\n",
    "      10: the data are properly collected and they are sufficient\n",
    "\n",
    "    -----------------------------------\n",
    "    Data Exploration:\n",
    "    -----------------------------------\n",
    "    4. How well did the team solve the following task:\n",
    "    \n",
    "    (1) Finding the most popular business categories (5 points):\n",
    "       0: missing answer\n",
    "       1: okay, but with major problems\n",
    "       3: good, but with minor problems\n",
    "       5: perfect\n",
    "    \n",
    "    (2) Find the most popular business objects (5 points)\n",
    "       0: missing answer\n",
    "       1: okay, but with major problems\n",
    "       3: good, but with minor problems\n",
    "       5: perfect\n",
    "    \n",
    "    -----------------------------------\n",
    "    The Solution\n",
    "    -----------------------------------\n",
    "    5.  how well did the team describe the solution they used to solve the problem? (10 points)\n",
    "       0: not clear\n",
    "       2: I can barely understand\n",
    "       4: okay, can be improved\n",
    "       6: good, but can be improved\n",
    "       8: very good\n",
    "       10: crystal clear\n",
    "       \n",
    "    6. how well is the solution in solving the problem? (10 points)\n",
    "       0: not relevant\n",
    "       2: barely relevant to the problem\n",
    "       4: okay solution, but there is an easier solution.\n",
    "       6: good, but can be improved\n",
    "       8: very good, but solution is simple/old\n",
    "       10: innovative and technically sound\n",
    "       \n",
    "    7. how well did the team implement the solution in python? (10 points)\n",
    "       0: the code is not relevant to the solution proposed\n",
    "       2: the code is barely understandable, but not relevant\n",
    "       4: okay, the code is clear but incorrect\n",
    "       6: good, the code is correct, but with major errors\n",
    "       8: very good, the code is correct, but with minor errors\n",
    "      10: perfect \n",
    "   \n",
    "    -----------------------------------\n",
    "    The Results\n",
    "    -----------------------------------\n",
    "     8.  How well did the team present the results they found in the data? (10 points)\n",
    "       0: not clear\n",
    "       2: I can barely understand\n",
    "       4: okay, can be improved\n",
    "       6: good, but can be improved\n",
    "       8: very good\n",
    "      10: crystal clear\n",
    "       \n",
    "     9.  How do you think of the results they found in the data?  (5 points)\n",
    "       0: not clear\n",
    "       1: likely to be wrong\n",
    "       2: okay, maybe wrong\n",
    "       3: good, but can be improved\n",
    "       4: make sense, but not interesting\n",
    "       5: make sense and very interesting\n",
    "     \n",
    "    -----------------------------------\n",
    "    The Presentation\n",
    "    -----------------------------------\n",
    "    10. How all the different parts (data, problem, solution, result) fit together as a coherent story?  \n",
    "       0: they are irrelevant\n",
    "       1: I can barely understand how they are related to each other\n",
    "       2: okay, the problem is good, but the solution doesn't match well, or the problem is not solvable.\n",
    "       3: good, but the results don't make much sense in the context\n",
    "       4: very good fit, but not exciting (the storyline can be improved/polished)\n",
    "       5: a perfect story\n",
    "      \n",
    "    11. Did the presenter make good use of the 10 minutes for presentation?  \n",
    "       0: the team didn't present\n",
    "       1: bad, barely finished a small part of the talk\n",
    "       2: okay, barely finished most parts of the talk.\n",
    "       3: good, finished all parts of the talk, but some part is rushed\n",
    "       4: very good, but the allocation of time on different parts can be improved.\n",
    "       5: perfect timing and good use of time      \n",
    "\n",
    "    12. How well do you think of the presentation (overall quality)?  \n",
    "       0: the team didn't present\n",
    "       1: bad\n",
    "       2: okay\n",
    "       3: good\n",
    "       4: very good\n",
    "       5: perfect\n",
    "\n",
    "\n",
    "    -----------------------------------\n",
    "    Overall: \n",
    "    -----------------------------------\n",
    "    13. How many points out of the 100 do you give to this project in total?  Please don't worry about the absolute scores, we will rescale the final grading according to the performance of all teams in the class.\n",
    "    Total score:\n",
    "    \n",
    "    14. What are the strengths of this project? Briefly, list up to 3 strengths.\n",
    "       1: \n",
    "       2:\n",
    "       3:\n",
    "    \n",
    "    15. What are the weaknesses of this project? Briefly, list up to 3 weaknesses.\n",
    "       1:\n",
    "       2:\n",
    "       3:\n",
    "    \n",
    "    16. Detailed comments and suggestions. What suggestions do you have for this project to improve its quality further.\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ---------------------------------\n",
    "    Your Vote: \n",
    "    ---------------------------------\n",
    "    1. [Overall Quality] Between the two submissions that you are reviewing, which team would you vote for a better score?  (5 bonus points)\n",
    "        0: I vote the other team is better than this team\n",
    "        5: I vote this team is better than the other team \n",
    "        \n",
    "    2. [Presentation] Among all the teams in the presentation, which team do you think deserves the best presentation award for this case study?  \n",
    "        1: Team 1\n",
    "        2: Team 2\n",
    "        3: Team 3\n",
    "        4: Team 4\n",
    "        5: Team 5\n",
    "        6: Team 6\n",
    "        7: Team 7\n",
    "        8: Team 8\n",
    "        9: Team 9\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
